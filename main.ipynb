{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c38e78",
   "metadata": {},
   "source": [
    "# Siamese CNN for Signature Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d41e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163dee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"[DEBUG] Using mixed precision policy:\", mixed_precision.global_policy())\n",
    "\n",
    "# Enable memory growth so TF does not grab all VRAM at once\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"[DEBUG] Enabled memory growth for {len(gpus)} GPU(s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"[DEBUG] No GPU detected, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8766c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"dataset_root\": \"./dataset\",\n",
    "    \"img_height\": 120,\n",
    "    \"img_width\": 200,\n",
    "    \"batch_size\": 8,          \n",
    "    \"epochs\": 40,             # train longer\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"margin\": 1.0,\n",
    "\n",
    "    \"num_train_writers\": 80,  \n",
    "    \"num_test_writers\": 10,\n",
    "    \"max_pairs_per_writer\": None,\n",
    "\n",
    "    \"models_root\": \"./models\",\n",
    "    \"experiment_name\": \"siamese_experiment\",\n",
    "    \"random_seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    print(f\"[DEBUG] Seeding everything with {seed}\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[DEBUG] Creating directory: {path}\")\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_experiment_dir(root: str, name: str) -> str:\n",
    "    ensure_dir(root)\n",
    "    idx = 1\n",
    "    while True:\n",
    "        p = os.path.join(root, f\"{name}_{idx:02d}\")\n",
    "        if not os.path.exists(p):\n",
    "            os.makedirs(p)\n",
    "            print(f\"[DEBUG] Created experiment directory: {p}\")\n",
    "            return p\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_EXTS = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab51d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(path: str, H: int, W: int):\n",
    "    imgs = []\n",
    "    print(f\"[DEBUG] Loading images from folder: {path}\")\n",
    "    for fname in sorted(os.listdir(path)):\n",
    "        if not fname.lower().endswith(VALID_EXTS):\n",
    "            continue\n",
    "        full = os.path.join(path, fname)\n",
    "        img = cv2.imread(full, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"[WARN] Cannot read {full}, skipping.\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (W, H))\n",
    "        img = img.astype(\"float32\") / 255.0\n",
    "        imgs.append(img)\n",
    "    print(f\"[DEBUG] Loaded {len(imgs)} images from {path}\")\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root: str, H: int, W: int):\n",
    "    if not os.path.isdir(root):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {root}\")\n",
    "\n",
    "    print(f\"[INFO] Loading dataset from {root}\")\n",
    "    writers = {}\n",
    "\n",
    "    for name in sorted(os.listdir(root)):\n",
    "        folder = os.path.join(root, name)\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "\n",
    "        if name.endswith(\"_forg\"):\n",
    "            writer_id = name[:-5]\n",
    "            mode = \"forg\"\n",
    "        else:\n",
    "            writer_id = name\n",
    "            mode = \"genuine\"\n",
    "\n",
    "        imgs = load_images_from_folder(folder, H, W)\n",
    "        if not imgs:\n",
    "            continue\n",
    "\n",
    "        if writer_id not in writers:\n",
    "            writers[writer_id] = {\"genuine\": [], \"forg\": []}\n",
    "\n",
    "        writers[writer_id][mode].extend(imgs)\n",
    "        print(\n",
    "            f\"[DEBUG] Writer {writer_id}, mode={mode}, total now: \"\n",
    "            f\"{len(writers[writer_id]['genuine'])} genuine, \"\n",
    "            f\"{len(writers[writer_id]['forg'])} forged\"\n",
    "        )\n",
    "\n",
    "    valid = {}\n",
    "    for w, d in writers.items():\n",
    "        g_count = len(d[\"genuine\"])\n",
    "        f_count = len(d[\"forg\"])\n",
    "        if g_count >= 2 and f_count >= 1:\n",
    "            valid[w] = d\n",
    "        else:\n",
    "            print(f\"[WARN] Skipping writer {w}: {g_count} genuine, {f_count} forged\")\n",
    "\n",
    "    print(f\"[INFO] Total writers found: {len(writers)}\")\n",
    "    print(f\"[INFO] Writers usable (>=2 genuine, >=1 forged): {len(valid)}\")\n",
    "\n",
    "    if len(valid) < 2:\n",
    "        raise RuntimeError(\"Not enough valid writers for writer-independent training.\")\n",
    "\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_writers(\n",
    "    writers: dict,\n",
    "    num_train: int,\n",
    "    num_test: int | None,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    all_ids = list(writers.keys())\n",
    "    total = len(all_ids)\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(all_ids)\n",
    "\n",
    "    # ensure we have at least 1 test writer\n",
    "    if num_train >= total:\n",
    "        num_train = max(total - 1, 1)\n",
    "\n",
    "    if num_test is None:\n",
    "        num_test = total - num_train\n",
    "    else:\n",
    "        num_test = min(num_test, total - num_train)\n",
    "        if num_test <= 0:\n",
    "            raise ValueError(\n",
    "                f\"num_test_writers={num_test} is not valid for total={total}, num_train={num_train}\"\n",
    "            )\n",
    "\n",
    "    train_ids = all_ids[:num_train]\n",
    "    test_ids = all_ids[num_train:num_train + num_test]\n",
    "\n",
    "    print(f\"[INFO] Writer split:\")\n",
    "    print(f\"       Total writers : {total}\")\n",
    "    print(f\"       Train writers : {len(train_ids)}\")\n",
    "    print(f\"       Test writers  : {len(test_ids)}\")\n",
    "\n",
    "    return train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(\n",
    "    writers: dict,\n",
    "    ids,\n",
    "    max_pairs_per_writer: int | None,\n",
    "    rng: random.Random\n",
    "):\n",
    "    X1, X2, y = [], [], []\n",
    "\n",
    "    print(\"[INFO] Generating pairs...\")\n",
    "    for wid in ids:\n",
    "        g = writers[wid][\"genuine\"]\n",
    "        f = writers[wid][\"forg\"]\n",
    "        print(f\"[DEBUG] Writer {wid}: {len(g)} genuine, {len(f)} forged\")\n",
    "\n",
    "        # Positive pairs: all distinct genuine-genuine pairs\n",
    "        pos = [(g[i], g[j]) for i in range(len(g)) for j in range(i + 1, len(g))]\n",
    "        # Negative pairs: genuine vs forged\n",
    "        neg = [(gi, fj) for gi in g for fj in f]\n",
    "\n",
    "        rng.shuffle(pos)\n",
    "        rng.shuffle(neg)\n",
    "\n",
    "        if not pos or not neg:\n",
    "            print(f\"[WARN] Writer {wid} has no pos or neg pairs, skipping.\")\n",
    "            continue\n",
    "\n",
    "        if max_pairs_per_writer is None:\n",
    "            use = min(len(pos), len(neg))\n",
    "        else:\n",
    "            use = min(max_pairs_per_writer, len(pos), len(neg))\n",
    "\n",
    "        pos = pos[:use]\n",
    "        neg = neg[:use]\n",
    "\n",
    "        for a, b in pos:\n",
    "            X1.append(a)\n",
    "            X2.append(b)\n",
    "            y.append(1.0)\n",
    "\n",
    "        for a, b in neg:\n",
    "            X1.append(a)\n",
    "            X2.append(b)\n",
    "            y.append(0.0)\n",
    "\n",
    "        print(f\"[DEBUG] Writer {wid}: using {use} positive and {use} negative pairs\")\n",
    "\n",
    "    if not X1:\n",
    "        raise RuntimeError(\"No pairs generated. Check dataset and writer IDs.\")\n",
    "\n",
    "    # store as float16 to reduce memory (with mixed precision this is fine)\n",
    "    X1 = np.array(X1, dtype=\"float16\")[..., np.newaxis]\n",
    "    X2 = np.array(X2, dtype=\"float16\")[..., np.newaxis]\n",
    "    y = np.array(y, dtype=\"float32\")\n",
    "\n",
    "    print(f\"[INFO] Total pairs generated: {len(y)}\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(\n",
    "        f\"[INFO] Label distribution (1=genuine, 0=forged): \"\n",
    "        f\"{dict(zip(unique.tolist(), counts.tolist()))}\"\n",
    "    )\n",
    "    print(f\"[DEBUG] X1 shape: {X1.shape}, X2 shape: {X2.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    approx_mem = (X1.nbytes + X2.nbytes + y.nbytes) / (1024 ** 2)\n",
    "    print(f\"[DEBUG] Approx dataset memory (X1+X2+y): {approx_mem:.2f} MB\")\n",
    "\n",
    "    return X1, X2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrn(x):\n",
    "    return tf.nn.local_response_normalization(\n",
    "        x,\n",
    "        depth_radius=2,\n",
    "        bias=2.0,\n",
    "        alpha=1e-4,\n",
    "        beta=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    print(f\"[DEBUG] Building base network with input shape: {input_shape}\")\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(32, (7, 7), padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Dense embedding\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Final L2 normalization ONLY here\n",
    "    x = Lambda(lambda t: tf.nn.l2_normalize(t, axis=1))(x)\n",
    "\n",
    "    base = Model(inp, x, name=\"BaseNetwork\")\n",
    "    print(\"[DEBUG] Base network summary:\")\n",
    "    base.summary(print_fn=lambda s: print(\"[DEBUG] \" + s))\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    a, b = vects\n",
    "    # do distance in float32 to keep numerics stable with mixed precision\n",
    "    diff = tf.cast(a, tf.float32) - tf.cast(b, tf.float32)\n",
    "    sq = K.square(diff)\n",
    "    dist = K.sqrt(K.maximum(K.sum(sq, axis=1, keepdims=True), K.epsilon()))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(margin: float):\n",
    "    m = tf.cast(margin, tf.float32)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_f = tf.cast(y_true, tf.float32)\n",
    "        y_pred_f = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        pos = y_true_f * K.square(y_pred_f)\n",
    "        neg = (1.0 - y_true_f) * K.square(K.maximum(m - y_pred_f, 0.0))\n",
    "        return K.mean(pos + neg)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c73796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese(input_shape, margin: float, lr: float):\n",
    "    print(f\"[DEBUG] Building siamese model with margin={margin}, lr={lr}\")\n",
    "    base = build_base_network(input_shape)\n",
    "\n",
    "    i1 = Input(shape=input_shape, name=\"input_1\")\n",
    "    i2 = Input(shape=input_shape, name=\"input_2\")\n",
    "\n",
    "    e1 = base(i1)\n",
    "    e2 = base(i2)\n",
    "\n",
    "    # distance output as float32 to keep loss numerically stable in mixed precision\n",
    "    dist = Lambda(\n",
    "        euclidean_distance,\n",
    "        name=\"distance\",\n",
    "        dtype=\"float32\"\n",
    "    )([e1, e2])\n",
    "\n",
    "    siamese = Model([i1, i2], dist, name=\"SiameseNetwork\")\n",
    "\n",
    "    opt = RMSprop(learning_rate=lr)\n",
    "    # Keras will wrap with LossScaleOptimizer automatically for mixed_float16\n",
    "\n",
    "    siamese.compile(\n",
    "        loss=contrastive_loss(margin),\n",
    "        optimizer=opt,\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Siamese model summary:\")\n",
    "    siamese.summary(print_fn=lambda s: print(\"[DEBUG] \" + s))\n",
    "\n",
    "    return siamese, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60850377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(history, path: str):\n",
    "    print(f\"[DEBUG] Saving learning curve to {path}\")\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distances(dists, labels, path: str):\n",
    "    print(f\"[DEBUG] Saving distance histogram to {path}\")\n",
    "    dists = np.asarray(dists).reshape(-1)\n",
    "    labels = np.asarray(labels).astype(int)\n",
    "\n",
    "    pos = dists[labels == 1]\n",
    "    neg = dists[labels == 0]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(pos, bins=30, alpha=0.6, label=\"genuine\")\n",
    "    plt.hist(neg, bins=30, alpha=0.6, label=\"forged\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ca638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, aucv, path: str):\n",
    "    print(f\"[DEBUG] Saving ROC curve to {path}\")\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={aucv:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\", label=\"random\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm, path: str):\n",
    "    print(f\"[DEBUG] Saving confusion matrix to {path}\")\n",
    "    plt.figure()\n",
    "    im = plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.colorbar(im)\n",
    "    plt.xticks([0, 1], [\"forged\", \"genuine\"])\n",
    "    plt.yticks([0, 1], [\"forged\", \"genuine\"])\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cfg: dict):\n",
    "    tf.keras.backend.clear_session()\n",
    "    seed_everything(cfg[\"random_seed\"])\n",
    "\n",
    "    exp_dir = create_experiment_dir(cfg[\"models_root\"], cfg[\"experiment_name\"])\n",
    "    print(f\"[INFO] Experiment directory: {exp_dir}\")\n",
    "\n",
    "    writers = load_dataset(cfg[\"dataset_root\"], cfg[\"img_height\"], cfg[\"img_width\"])\n",
    "    train_ids, test_ids = split_writers(\n",
    "        writers,\n",
    "        cfg[\"num_train_writers\"],\n",
    "        cfg[\"num_test_writers\"],\n",
    "        seed=cfg[\"random_seed\"],\n",
    "    )\n",
    "\n",
    "    rng = random.Random(cfg[\"random_seed\"])\n",
    "\n",
    "    print(\"[INFO] Generating training pairs...\")\n",
    "    X1_train, X2_train, y_train = generate_pairs(\n",
    "        writers, train_ids, cfg[\"max_pairs_per_writer\"], rng\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Generating test pairs...\")\n",
    "    X1_test, X2_test, y_test = generate_pairs(\n",
    "        writers, test_ids, cfg[\"max_pairs_per_writer\"], rng\n",
    "    )\n",
    "\n",
    "    # free raw images to reduce CPU RAM usage\n",
    "    del writers\n",
    "    gc.collect()\n",
    "\n",
    "    input_shape = (cfg[\"img_height\"], cfg[\"img_width\"], 1)\n",
    "    siamese, base = build_siamese(input_shape, cfg[\"margin\"], cfg[\"learning_rate\"])\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=8,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    print(\"[INFO] Starting training...\")\n",
    "    history = siamese.fit(\n",
    "        [X1_train, X2_train],\n",
    "        y_train,\n",
    "        validation_data=([X1_test, X2_test], y_test),\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        epochs=cfg[\"epochs\"],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Predicting distances on test pairs...\")\n",
    "    dists = siamese.predict([X1_test, X2_test]).reshape(-1)\n",
    "    # move to float32 on CPU for metrics\n",
    "    dists = dists.astype(\"float32\")\n",
    "    print(\n",
    "        f\"[DEBUG] Distances stats -> \"\n",
    "        f\"min: {dists.min():.4f}, max: {dists.max():.4f}, \"\n",
    "        f\"mean: {dists.mean():.4f}, std: {dists.std():.4f}\"\n",
    "    )\n",
    "\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # ROC (note: smaller distance => more likely genuine)\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, -dists)\n",
    "    aucv = auc(fpr, tpr)\n",
    "    print(f\"[INFO] ROC AUC on test pairs: {aucv:.4f}\")\n",
    "\n",
    "    # Threshold sweep for balanced accuracy\n",
    "    thresholds = np.linspace(dists.min(), dists.max(), 500)\n",
    "    best_acc = -1.0\n",
    "    best_th = None\n",
    "\n",
    "    print(\"[INFO] Sweeping thresholds for balanced accuracy...\")\n",
    "    for th in thresholds:\n",
    "        pred = (dists < th).astype(int)\n",
    "        cm = confusion_matrix(y_true, pred, labels=[0, 1])\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # degenerate case; skip\n",
    "            continue\n",
    "        tpr_val = tp / (tp + fn + 1e-8)\n",
    "        tnr_val = tn / (tn + fp + 1e-8)\n",
    "        acc = 0.5 * (tpr_val + tnr_val)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_th = th\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Best threshold: {best_th:.4f}, \"\n",
    "        f\"best balanced accuracy: {best_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    final_pred = (dists < best_th).astype(int)\n",
    "    final_cm = confusion_matrix(y_true, final_pred, labels=[0, 1])\n",
    "\n",
    "    # Save models\n",
    "    siamese_path = os.path.join(exp_dir, \"siamese_model.h5\")\n",
    "    base_path = os.path.join(exp_dir, \"embedding_model.h5\")\n",
    "    print(f\"[DEBUG] Saving siamese model to {siamese_path}\")\n",
    "    print(f\"[DEBUG] Saving base model to {base_path}\")\n",
    "    siamese.save(siamese_path)\n",
    "    base.save(base_path)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"best_threshold\": float(best_th),\n",
    "        \"best_balanced_accuracy\": float(best_acc),\n",
    "        \"roc_auc\": float(aucv),\n",
    "        \"confusion_matrix\": final_cm.tolist(),\n",
    "        \"num_train_pairs\": int(len(y_train)),\n",
    "        \"num_test_pairs\": int(len(y_test)),\n",
    "    }\n",
    "\n",
    "    metrics_path = os.path.join(exp_dir, \"metrics.json\")\n",
    "    print(f\"[DEBUG] Saving metrics to {metrics_path}\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Save plots\n",
    "    plot_curve(history, os.path.join(exp_dir, \"learning_curve.png\"))\n",
    "    plot_distances(dists, y_true, os.path.join(exp_dir, \"distance_hist.png\"))\n",
    "    plot_roc(fpr, tpr, aucv, os.path.join(exp_dir, \"roc_curve.png\"))\n",
    "    plot_cm(final_cm, os.path.join(exp_dir, \"confusion_matrix.png\"))\n",
    "\n",
    "    print(\"[INFO] Finished experiment.\")\n",
    "    print(\"[INFO] Metrics:\", metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment(BASE_CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
